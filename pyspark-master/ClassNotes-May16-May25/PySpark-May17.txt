 
 Agenda (PySpark)
 ----------------
   -> Spark - Basic & Architecture
   -> Spark Core API
	-> RDD Transformations and Actions
	-> Shared Variables
   -> Spark SQL
	-> DataFrame Operations
   -> Spark Streaming
	-> Strutured Streaming API
   -> Intro to Spark MLLib (based on time) 


  Materials  
  ---------

    -> PDF Presenations
    -> Code Modules
    -> Class Notes
    -> GibHub: https://github.com/ykanakaraju/pyspark


  Spark
  -----
	
    -> Spark is written is SCALA programming language.
  
     -> Is a open source distributed computing framework which performs in-memory computations
        on a cluster using simple programming constructs. 


        Cluster: Is a unified entity consisting of many nodes whose cumulative resources can be used to
                 distributed storage and processing of big data. 
		
		-> A cluster is managed by 'cluster manager'

     -> Spark is a polyglot
	   -> Supports Scala, Java, Python, R 

     -> Spark is a unified framework.
	
          Spark comes with multiple APIs for processing different analytics workloads using the same
	  execution engine. 

            	Batch Processing of unstructured data:	Spark Core (Low-level API)
		Batch Processing of structured data:  	Spark SQL
		Streaming analytics (real time):	Spark Streaming, Structure Streaming
	        Predictive analytics (ML):		Spark MLlib
		Graph Data Processing:			Spark GraphX
	   

   Spark Architecture
   ------------------

	1. Cluster Manager
		-> Jobs are submitted to CMs
		-> Allocates containers for lauching driver and executors on the cluster. 
		-> Spark supports multiple CMs
			-> local, standalone, YARN, Mesos, Kubernetes		

	2. Driver
		-> Master process
		-> Runs the SparkContext object
		-> Manages the user code and sends the tasks to the executors as per the code. 

		Deploy Modes:
		1. Client : default, driver runs on the client. 
		2. Cluster : driver runs on one of the nodes on the cluster.

	3. SparkContext
		-> Is an application context
		-> Starting point of execution
		-> Is the link between the driver and tasks running on the executors

	4. Executors
		-> receives the tasks from the Driver
		-> all tasks runs the same execution code but on different partitions of the data
		-> the status of tasks os reported to the driver.


   Getting started with PySpark
   -----------------------------

    1. Working in vLab:
	 -> Follow the instruction mentioned in the email.
	 -> You will login to a Windows server. There is "Instructions" document on the desktop.
	 -> Click on the "Oracle Virtualbox VM" and lauch the VM.
		
	 1.1 Lauch pyspark shell
		-> Open a terminal and type "pyspark"
	
         1.2 Install spyder
		-> This is the IDE for running PySpark applications. 
   
    2. Setting up PySpark environment on your local machine.  		
	
	-> Install "Anancoda Navigator"
		https://www.anaconda.com/products/distribution#windows
	-> Follow the instructions mentioned in the shared document
		https://github.com/ykanakaraju/pyspark/blob/master/Pyspark-JupyterNotebooks-Windows-Setup.pdf

    3. Signup for Databricks community edition account.
		
 	    Signup: https://databricks.com/try-databricks
	    Login: https://community.cloud.databricks.com/login.html



   Spark Core API & RDD (Resilient Distributed Dataset) 
   -----------------------------------------------------

    -> RDD is the fundamental data abstraction of Spark Core API

    -> RDD is a collection of distributed in-memory partitions.
	-> Partition is a collection of objects
       
    -> RDDs are immutable

    -> RDDs are lazily evaluated
	   -> Transformations doe no cause execution
	   -> Action commands cause execution. 

    -> RDDs are resilient
	-> RDD can create missing in-memory partitions at runtime (on the fly)


   How to create RDDs ?
   ------------------    
     Three ways to create an RDD:

	1. Create an RDD from some external data file.
		rdd1 = sc.textFile("E:\\Spark\\wordcount.txt", 4)

		default number of partitions = sc.defaultMinPartitions (is 2 if cores >= 2)
		rdd1 = sc.textFile("E:\\Spark\\wordcount.txt")   # partitions: 2

	2. Create an RDD from programmatic data
		rdd1 = sc.parallelize( range(1, 101), 3 )

		default number of partitions = sc.defaultParallelism (is equal to num of cores)
		rdd1 = sc.parallelize( range(1, 101))   # partitions: 4

	3. By applying transformations on existing RDDs

		rdd2 = rdd1.flatMap(lambda x: x.split(" "))


   What can we do with a RDD?
   --------------------------

	Two things:

	1. Transformations	
		-> Returns an RDD
		-> Does not cause execution
		-> Transformation only create RDD Lineage DAGs

        2. Actions
		-> Trigger execution of the RDD
		-> Produces output
		-> Converts the logical Plan into a physical execution plan.


   RDD Lineage DAG
   ---------------
    -> RDD Lineage DAG is a logical plan maintained by the Driver
    -> RDD Lineage contains the hierarchy of dependencies all the way from the very first RDD.
    -> RDD Lineage DAGs are create by transformations and data loading commands.
    -> Transformations does not cause execution

    rddFile = sc.textFile("E:\\Spark\\wordcount.txt", 4)
	lineage of rddFile :  (4) rddFile -> sc.textFile

    rdd2 = rddFile.flatMap(lambda x: x.split(" "))
	lineage of rdd2 :  (4) rdd2 -> rddFile.flatMap -> sc.textFile

    rdd3 = rdd2.map(lambda x: (x, 1))
	lineage of rdd3 : (4) rdd3 -> rdd2.map -> rddFile.flatMap -> sc.textFile

    rdd4 = rdd3.reduceByKey(lambda x, y: x + y)
	lineage of rdd4 : (4) rdd4 -> rdd3.reduceByKey -> rdd2.map -> rddFile.flatMap -> sc.textFile
	

  Types of Transformations
  ------------------------

	1. Narrow Transformations
	    -> Does not cause shuffling of the data from one partition to other partitions
	    -> Partition to partition transformations
	    -> The output RDD will have the same number of partitions as input RDD

	2. Wide Transformation
	    -> Causes shuffling of the data
	    -> One output partition may need data from multiple input partitions
	    -> The output RDD may have different number of partitions than input RDD


  Persistence
  -----------

    	rdd1 = sc.textFile( <file>, 4 )
	rdd2 = rdd1.t2( ... )
	rdd3 = rdd1.t3( ... )
	rdd4 = rdd3.t4( ... )
	rdd5 = rdd3.t5( ... )
	rdd6 = rdd5.t6( ... )
	rdd6.persist( StorageLevel.MEMORY_AND_DISK )   -> instaruction to spark to save rdd6 partitions.
	rdd7 = rdd6.t7( ... )

        rdd6.collect()

	lineage DAG of rdd6: (4) rdd6 -> rdd5.t6 -> rdd3.t5 -> rdd1.t3 -> sc.textFile
		tasks: [sc.textFile, t3, t5, t6]

	rdd7.collect()
	
	lineage DAG of rdd7: (4) rdd7 -> rdd6.t7
		tasks: [t7]

	rdd6.unpersist()

	Storage Levels
        --------------
	1. MEMORY_ONLY		: default, Memory Serialized 1x replicated
	2. MEMORY_AND_DISK	: Disk Memory Serialized 1x replicated
	3. DISK_ONLy		: Disk Serialized 1x replicated
	4. MEMORY_ONLY_2	: Memory Serialized 2x replicated
	5. MEMORY_AND_DISK_2	: Disk Memory Serialized 2x replicated

	Command
	-------
	-> rdd1.cache()   				-> in-memory
	-> rdd1.persist()   				-> in-memory
	-> rdd1.persist(StorageLevel.MEMORY_AND_DISK)	-> custom storage level
	
	-> rdd1.unpersist()


  Executor's memory structure 
  ----------------------------
	Reference URL: https://spark.apache.org/docs/latest/configuration.html

	Let us assume, we request executors with 10 GB RAM.
	
	-> Cluster Manager allocates exectors with 10.3 GB RAM

	1. Reserved Memory  : 300 MB
		-> Spark's internal usage. 

	2. Spark Memory (spark.memory.fraction: 0.6) => 6 GB
		-> Used for RDD execution and storage

		2.1 Execution Memory
			-> Used for execution of RDD tasks and creating RDD partitions. 

		2.2 Storage Memory (spark.memory.storageFraction = 0.5)  => 3 GB
			-> Used for RDD persistence and storing broadcast variables.

            -> Storage memory can not evict execution memory even if execution memory is
               using more than its 3 GB limit. It has to wait until more memory becomes 
	       available.

	    -> Execution memory can evict some partitions from storage, if it requires more
	       memory. But, it can evict only excess portion that is used by storage beyond its
 	       3 GB limit. 

	3. User Memory (1 - spark.memory.fraction = 0.4) => 4 GB
		-> Used for user code (Python/Scala/Java etc)

   
  RDD Transformations
  -------------------

  1. map		P: U -> V
			Object to object transformation
			input RDD: N objects, output RDD: N objects

	rddFile.map(lambda x: len(x.split(" "))).count()


  2. filter		P: U -> Boolean
			Selects the objects for which the function returns True
			input RDD: N objects, output RDD: <= N objects

	rddFile.filter(lambda x: len(x.split(" ")) > 8).collect()

  3. glom		P: None
			Returns one list object per partition with all the values of that partition.
			input RDD: N objects, output RDD: equal to number of partitions

		rdd1			rdd2 = rdd1.glom()
		P0: 2,1,3,2,5,6 -> glom -> P0: [2,1,3,2,5,6]
		P1: 7,4,3,2,8,5 -> glom -> P1: [7,4,3,2,8,5]
		P2: 9,0,1,8,3,7 -> glom -> P2: [9,0,1,8,3,7]
		
		rdd1.count() = 18 (int)    rdd2.count() = 3 (list)
	
		rdd1.glom().map(sum).collect()

   4. flatMap		P: U -> Iterable[V]
			flatMap flattens the iterable produced by the function. 			
			input RDD: N objects, output RDD: > N objects

		rddWords = rddFile.flatMap(lambda x: x.split(" "))

   5. mapPartitions		P: Iterable[U] -> Iterable[V]
				partition to partition transformation
				Each partition is taken as function input and the output of the fucntion will be
				output partition of the RDD
				
		rdd1		rdd2 = rdd1.mapPartitions(lambda p: [sum(p)] )  
		P0: 2,1,3,2,5,6 -> mapPartitions -> P0: 19
		P1: 7,4,3,2,8,5 -> mapPartitions -> P1: 29
		P2: 9,0,1,8,3,7 -> mapPartitions -> P2: 28

		rdd1.mapPartitions(lambda p : [sum(p)]).collect()

		rdd1.mapPartitions(lambda p: map(lambda x: x*10, p)).collect()
		

   6. mapPartitionsWithIndex  	P: Int, Iterable[U] -> Iterable[V] 
				Samilar to mapPartitions, but we get the partition-index as additional parameter.

		rdd1.mapPartitionsWithIndex(lambda i, p : [(i, sum(p))]).collect()
		rdd1.mapPartitionsWithIndex(lambda i, p: map(lambda x: (i, x*10), p)).collect()


   7. distinct			P: None, Optional: numPartitions
				Return distinct elements of the RDD
				input RDD: N objects, output RDD: <= N objects

		rddWords.distinct().collect()
		rddWords.distinct(5).collect()


   Types of RDDs
   -------------

	1. Generic RDDs:   RDD[U]
	2. Pair RDDs:	   RDD[(K, V)]

   8. mapValues		P: U -> V
			Applied only on Pair RDDs
			Transforms the value part only by applying the function. 

		rdd3.mapValues(lambda x: x[1]).collect()

   9. sortBy		P: U -> V, Optional: ascending (True/False), numPartition
			The elements of the RDD are sorted based on the function output

		rdd1.sortBy(lambda x: x%3).collect()
		rdd1.sortBy(lambda x: x%3, False).collect()
		rdd1.sortBy(lambda x: x%3, False, 2).collect()


   10. groupBy









 




















